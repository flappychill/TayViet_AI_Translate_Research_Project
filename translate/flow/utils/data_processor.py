import pandas as pd
import os
class DataProcessor:
    def __init__(self, output_dir="../data"):
     
        self.urls = [            "https://docs.google.com/spreadsheets/d/1_lcoKY79oM7WsDCM1-yCQ24kZzL99NU-/export?format=csv",            "https://docs.google.com/spreadsheets/d/1lvpUQC4VWqYFHGmsqbeC-pVeTnmltdah/export?format=csv",            "https://docs.google.com/spreadsheets/d/1jnX96jO81FZVRJQ6iy-b_7SMtFztOtIt/export?format=csv"        ]
        self.output_dir = output_dir
        self.dataframes = []
        self.rows_per_file = []
        self.rows_removed = 0
        self.removed_rows_info = []
        self.duplicates_removed = 0
        self.merged_df = None
        os.makedirs(self.output_dir, exist_ok=True)
    def load_and_clean_data(self):
       
        for url in self.urls:
            df = pd.read_csv(url)
            if "ti·∫øng tay" in df.columns and "ti·∫øng vi·ªát" in df.columns:
                initial_rows = df.shape[0]
                self.rows_per_file.append(initial_rows)
                invalid_rows = df[(df["ti·∫øng tay"].isna() & ~df["ti·∫øng vi·ªát"].isna()) |                                  (~df["ti·∫øng tay"].isna() & df["ti·∫øng vi·ªát"].isna())]
                self.rows_removed += invalid_rows.shape[0]
                self.removed_rows_info.append((url, invalid_rows.index.tolist()))
                df = df.drop(invalid_rows.index)
                self.dataframes.append(df[["ti·∫øng tay", "ti·∫øng vi·ªát"]])
        self.merged_df = pd.concat(self.dataframes, ignore_index=True)
        initial_merged_rows = self.merged_df.shape[0]
        self.merged_df = self.merged_df.drop_duplicates(subset=["ti·∫øng tay"]).drop_duplicates(subset=["ti·∫øng vi·ªát"])
        self.duplicates_removed = initial_merged_rows - self.merged_df.shape[0]
    def save_clean_data(self, output_filename="final.csv"):
       
        output_path = os.path.join(self.output_dir, output_filename)
        if self.merged_df is not None:
            self.merged_df.to_csv(output_path, index=False)
        else:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u!")
    def extract_sentences(self, column_name="ti·∫øng tay", output_filename="tay_data.txt"):
       
        if self.merged_df is None or column_name not in self.merged_df.columns:
            raise ValueError(f"‚ö†Ô∏è C·ªôt '{column_name}' kh√¥ng t·ªìn t·∫°i ho·∫∑c d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c load!")
        self.merged_df[column_name] = self.merged_df[column_name].str.normalize('NFKC').str.strip()
        sentences = self.merged_df[column_name].dropna().str.strip().unique()
        output_path = os.path.join(self.output_dir, output_filename)
        with open(output_path, "w", encoding="utf-8") as f:
            for sentence in sentences:
                if sentence:
                    f.write(sentence + "\n")
    def print_summary(self):
        """
        In t√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu.
        """
           # print("üìä T√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu:")
        # print("T·ªïng s·ªë file:", len(self.urls))
        # print("S·ªë h√†ng trong t·ª´ng file ban ƒë·∫ßu:", self.rows_per_file)
        # print("S·ªë h√†ng b·ªã x√≥a do ch·ªâ c√≥ gi√° tr·ªã ·ªü m·ªôt c·ªôt:", self.rows_removed)
        # print("S·ªë h√†ng tr√πng l·∫∑p b·ªã x√≥a:", self.duplicates_removed)
        # print("S·ªë h√†ng c√≤n l·∫°i sau khi x·ª≠ l√Ω:", self.merged_df.shape[0] if self.merged_df is not None else 0)
        # print("ƒê·ªãa ch·ªâ c·ªßa c√°c h√†ng b·ªã x√≥a:")
        # for file, rows in self.removed_rows_info:
        #     print(f"File: {file}, H√†ng b·ªã x√≥a: {rows}")
